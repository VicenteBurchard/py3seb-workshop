{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Corregistration of UAV mosaics\n",
    "subject: Tutorial\n",
    "subtitle: Notebook that shows a prototype method to automatically corregister airborne/UAV orthomomosaics\n",
    "short_title: UAV corregistration\n",
    "authors:\n",
    "  - name: HÃ©ctor Nieto\n",
    "    affiliations:\n",
    "      - Instituto de Ciencias Agrarias, ICA\n",
    "      - CSIC\n",
    "    orcid: 0000-0003-4250-6424\n",
    "    email: hector.nieto@ica.csic.es\n",
    "  - name: Benjamin Mary\n",
    "    affiliations:\n",
    "      - Insituto de Ciencias Agrarias\n",
    "      - CSIC\n",
    "    orcid: 0000-0001-7199-2885\n",
    "license: CC-BY-SA-4.0\n",
    "keywords: UAV, TSEB, LST-NDVI space\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "This interactive Jupyter Notebook has the objective of showing one prototype method that automates the search of Ground Control Points between two UAV mosaics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "Read carefully all the text and follow the instructions.\n",
    "\n",
    "Once each section is read, run the jupyter code cell underneath (marked as `In []`) by clicking the icon `Run`, or pressing the keys SHIFT+ENTER of your keyboard. A graphical interface will then display, which allows you to interact with and perform the assigned tasks.\n",
    "\n",
    "To start, please run the following cell to import all the packages required for this notebook. Once you run the cell below, an acknowledgement message, stating all libraries were correctly imported, should be printed on screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from pathlib import Path\n",
    "from ipywidgets import interact, interactive, fixed, widgets\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "print(\"Libraries imported correctly, you can continue to the next cells\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corregistering orthomosaics\n",
    "In the previous exercise we saw how important is having well collocated UAV mosaicks between the TIR and spectral cameras. Not only for accurately extracting the soil and canopy temperatures, but for later be able to robustly parse the different canopy tratis and radiometric temperatures in TSEB. \n",
    "\n",
    "Althouh both phototgrametric software and UAV avionics/payloads have evolve a lot in the last years, it is not uncommon that the different orthomosaics generated from the different payload cameras do not perfectly match. This would involve the manual tasks of adding Ground Control Points, either with targets placed on ground or by indentifying distinctive patters in each mosaick.\n",
    "\n",
    ":::{important}\n",
    "This task can be tedious and time consuming, thus it woulc preclude the use of UAV imager in operational and/or near-real-time services.\n",
    ":::\n",
    "\n",
    "# The [gcp_tools in GitHub airborne_tools package](https://github.com/hectornieto/airborne_tools/blob/5db17192e638c2745dea5d918b9dcaffd05a14cf/airborne_tools/gcp_tools.py#L18)\n",
    "For that reason, we have developed a prototype that aims to automatize the corregistration of orthomosaics. This prototype aims to collocate an orthomosaic[^1] (from now on we will call it the `slave`) over another orthomosaic or image that is considered as `reference`.\n",
    "\n",
    "[^1]: Or any other geospatial raster image such as satellite imagery.\n",
    "\n",
    ":::{hint}\n",
    "The `reference` image can be a well georreferenced orthomosaic such as an RGB UAV mosaic, an aerial orthoimage or any other dataset. Indeed the **scale/resolution between the `reference` and the `slave` mosaic to be collocated does not need to be the same**\n",
    ":::\n",
    "\n",
    "Basically we are using SIFT {cite:p}`https://doi.org/10.1023/B:VISI.0000029664.99615.94` algorithm to find relevant features in each image, followed by FLANN {cite:p}`https://doi.org/10.1109/TPAMI.2014.2321376` algorithm to evaluate the similarities of features between both images (reference and slave) and find feature mathing in both images, which will be the ones considered as potential Ground Control Points (`GCP`s).\n",
    "\n",
    ":::{note}\n",
    ":class:drowpdown\n",
    "Indeed SIFT+FLANN are the typical algorithms that photogrammetric software uses to peform the matching and overlapping between snapshots, pior to building the photogrammetric point cloud.\n",
    ":::\n",
    "\n",
    "## Dataset\n",
    "For this exercise we will use the test example of [airborne_tools package](https://github.com/hectornieto/airborne_tools). The data is already located at the [./input/UAV](./input/UAV) folder. It consists of a UAV flight over an experimental vineyard located near Madrid (Spain) that acquired (among others):\n",
    "* A multispectral image ([](./input/UAV/Sequoia_vnir_20220916.tif)) with the following bands:\n",
    "\n",
    "  1. Green\n",
    "  2. Red\n",
    "  3. Red-edge\n",
    "  4. Near Infrared\n",
    "* A thermal image ([](./input/UAV/tir_odm_20220916.tif))with temperatures in Kelvin, scaled by a factor of 100\n",
    "\n",
    "We recommend you to open both images in QGIS to better browse them and be aware of the lack of corregistration between mosaics.\n",
    "\n",
    ":::{hint}\n",
    "You can use the cars/pannels in the upper part of the scene as reference.\n",
    ":::\n",
    "\n",
    "Even the UAV was equipped with a RTK differential GPS and Ground Control Points were place on ground, you can still see some some displacements between mosaics.\n",
    "\n",
    "## Preprocess the mosaics\n",
    "As an optional step we could do is to preprocess the mosaics. Either the `reference` or the `slave`. This could be usefull for multidimensional imagery such as multispectral/hyperspectral data, since SIFT algorithm only works for single-band grayscale or RGB pictures. \n",
    "\n",
    "For this case we will reduce the multispectral `reference` scene to a single band brightness image, by applying a Principal Component Analysis, and selecting the first PCA, since this is the one that will "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airborne_tools import image_preprocessing as img\n",
    "\n",
    "# Set the input folder location and raw VNIR and output PCA single-band image\n",
    "workdir = Path()\n",
    "test_dir = workdir / \"input\" / \"UAV\"\n",
    "\n",
    "# Set the path to the input LST and VNIR images\n",
    "lst_file = test_dir / 'tir_odm_20220916.tif'\n",
    "vnir_image = test_dir / 'Sequoia_vnir_20220916.tif'\n",
    "pca_image = test_dir / 'Sequoia_vnir_20220916_PC1.tif'\n",
    "\n",
    "# Set the nodata of the VNIR imager\n",
    "no_data=4294967296\n",
    "\n",
    "# We will use all the VNIR bands (4) to create the PCA image\n",
    "vnir_bands = [0, 1, 2, 3]\n",
    "\n",
    "# And we will save only the first PC band, considering that this component \n",
    "pca_components = 1\n",
    "# We need to reduce the dimensionality of the master image to a single grayscale band.\n",
    "# We therefore apply a PCA reduction to get a grayscale image combining all spectral bands\n",
    "if not pca_image.exists():\n",
    "    img.pca(vnir_image,\n",
    "            no_data=no_data,\n",
    "            use_bands=vnir_bands,\n",
    "            pca_components=pca_components,\n",
    "            outfile=pca_image,\n",
    "            normalize=True)\n",
    "\n",
    "print(f\"Created VNIR PCA image at {pca_image}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{seealso}\n",
    ":class:dropdown\n",
    "You can check the [PCA image reduction GitHub source code](https://github.com/hectornieto/airborne_tools/blob/5db17192e638c2745dea5d918b9dcaffd05a14cf/airborne_tools/image_preprocessing.py#L9)\n",
    ":::\n",
    "\n",
    "### Visualize both grayscale images\n",
    "You can now visualized the grayscale images for both the `reference` VNIR PCA1 and the `slave` LST.\n",
    "\n",
    ":::{note}\n",
    "You can also use QGIS to better visualize both images and confirm the corregistration issues between mosaics.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from osgeo import gdal\n",
    "from bokeh.plotting import *\n",
    "from bokeh import palettes as pal\n",
    "from bokeh.models.mappers import LinearColorMapper\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.resources import INLINE\n",
    "output_notebook(resources=INLINE)\n",
    "\n",
    "# Open and read the LST file\n",
    "fid = gdal.Open(lst_file, gdal.GA_ReadOnly)\n",
    "lst = fid.GetRasterBand(1).ReadAsArray().astype(float)\n",
    "# Set LST NaN\n",
    "lst_no_data = 65535\n",
    "lst[lst == lst_no_data] = np.nan\n",
    "# Open and read the NDVI file\n",
    "fid = gdal.Open(pca_image, gdal.GA_ReadOnly)\n",
    "pca = fid.GetRasterBand(1).ReadAsArray()\n",
    "master_geo = fid.GetGeoTransform()\n",
    "del fid\n",
    "\n",
    "rows, cols = int(0.3 * lst.shape[0]), int(0.3 * lst.shape[1])\n",
    "s1 = figure(title=\"LST\", width=cols, height=rows, x_range=[0, cols], y_range=[0, rows])\n",
    "s1.axis.visible = False\n",
    "s1.image(image=[np.flipud(lst)], x=[0], y=[0], dw=cols, dh=rows)\n",
    "s2= figure(title=\"VNIR PCA1\", width=cols, height=rows, x_range=s1.x_range, y_range=s1.y_range)\n",
    "s2.axis.visible = False\n",
    "s2.image(image=[np.flipud(pca)], x=[0], y=[0], dw=[cols], dh=[rows])\n",
    "\n",
    "p = gridplot([[s1], [s2]], toolbar_location='above')\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look that the brightness patters seems to overal match, which will help SIFT+FLANN to find the matheces.\n",
    "For instance,  within the grapevine that the brightest vines in the the LST (warmer temperatures) match with the lowest brighness in the PCA, these areas were under a more stressed deficit irrigation and thus both temperatures. However, some other areas have opposite brighness, such as the calibration pannels placed on top of the scenes, the warmest pannels (brightest in LST) are however the darkest (black painted) in the VNIR PCA1.\n",
    "\n",
    "## Run SIFT + FLANN\n",
    "Now that we have the data preprocess, we can run our SIFT and FLANN algorithms to find common features that we will be considered as potential GCPs. For this task we will use the powerful [OpenvCV] library, available in Python[^2], which, among many others, contains SIFT and FLANN methods:\n",
    "\n",
    "[^2]: also in C++ and Java"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Python OpenCV\n",
    "import cv2\n",
    "# matching factor between keypoint descriptor\n",
    "match_factor = 0.80 \n",
    "\n",
    "# SIFT and FLANN needs to deal with 8bit images, so we need to rescale the floaging point input to 0-255 bit range\n",
    "master_scaled = img.scale_grayscale_image(pca, no_data=np.nan)\n",
    "slave_scaled = img.scale_grayscale_image(lst, no_data=np.nan)\n",
    "\n",
    "# Get the LST (slave) GDAL geotransform and projection\n",
    "fid = gdal.Open(lst_file, gdal.GA_ReadOnly)\n",
    "slave_geo = fid.GetGeoTransform()\n",
    "proj = fid.GetProjection()\n",
    "del fid\n",
    "# Get the VNIR (master) GDAL geotransform and projection\n",
    "fid = gdal.Open(pca_image, gdal.GA_ReadOnly)\n",
    "master_geo = fid.GetGeoTransform()\n",
    "proj = fid.GetProjection()\n",
    "del fid\n",
    "\n",
    "# Initiate SIFT detector\n",
    "detector = cv2.SIFT_create()\n",
    "# We use NORM distance measurement for SIFT\n",
    "norm_type = cv2.NORM_L1\n",
    "\n",
    "print(\"Finding features and their descriptors, this might take a while...\")\n",
    "kp_master, des_master = detector.detectAndCompute(master_scaled, None)\n",
    "kp_slave, des_slave = detector.detectAndCompute(slave_scaled, None)\n",
    "\n",
    "# Find matches between slave and master descriptors\n",
    "matcher = cv2.BFMatcher(norm_type)\n",
    "# Get the 2 best matches per feature\n",
    "matches = matcher.knnMatch(des_master, des_slave, k=2)\n",
    "\n",
    "print(f\"Found {len(matches)}, filtering by FLANN factor similarity of {match_factor}\")\n",
    "# Create a list of potential GCPs\n",
    "gcp_list = []\n",
    "good_matches = []\n",
    "for i, (m, n) in enumerate(matches):\n",
    "    # Only the most similar matches, based on FLANN match factor, are selected\n",
    "    if m.distance < match_factor * n.distance:\n",
    "        good_matches.append(m)\n",
    "        master_pt = np.float32(kp_master[m.queryIdx].pt)\n",
    "        # Get the projected map coordinates (X, Y) of the master\n",
    "        x_master, y_master = img.get_map_coordinates(float(master_pt[1]),\n",
    "                                                     float(master_pt[0]),\n",
    "                                                     master_geo)\n",
    "        # Get the image coordinates (row, col) of the slave\n",
    "        slave_pt = np.float32(kp_slave[m.trainIdx].pt)\n",
    "        gcp_list.append((x_master,\n",
    "                         y_master,\n",
    "                         float(slave_pt[1]),\n",
    "                         float(slave_pt[0])))\n",
    "\n",
    "print(f\"Found {len(gcp_list)} potential GCPs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the potential GCP\n",
    "We can now visualize all the matches found by SIFT+FLANN, according to the resemblance ratio we have just set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "#-- Draw matches\n",
    "img_matches = np.empty((max(slave_scaled.shape[0], master_scaled.shape[0]), \n",
    "                        slave_scaled.shape[1] + master_scaled.shape[1], 3), \n",
    "                       dtype=np.uint8)\n",
    "cv2.drawMatches(master_scaled, kp_master, slave_scaled, kp_slave, good_matches, img_matches,\n",
    "                matchesThickness=10, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "#-- Show detected matches\n",
    "plt.imshow(img_matches)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter GCP by translation distance\n",
    "Since we assume that a priori both orthomosaics were georreferenced, either the UAV avionics information, the use of in situ GCP, or both, we can consider that the displacment between the `slave` and the `master` should not be too large, and thus we could discard GCPs which translation vector is larger than a given distance\n",
    "\n",
    ":::{hint}\n",
    "For instance we can assume that the UAV GPS could have an unsistematic error of 10m and thus the displacement between mosaics should not be larger than this distance. \n",
    "\n",
    "Or we can just browse both mosaics and evaluate the observed maximum displacement between mosaics\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a maximum translation distance between slave and master\n",
    "dist_threshold = 10\n",
    "\n",
    "# Convert the list of potential GCPs to a numpy array to make computations more effective\n",
    "gcp_list = np.asarray(gcp_list)\n",
    "\n",
    "if len(gcp_list.shape) == 1:\n",
    "    gcp_list = gcp_list.reshape(1, -1)\n",
    "\n",
    "# Get the GCP map coordinates (X, Y) from the slave image coordinates\n",
    "x_slave, y_slave = img.get_map_coordinates(gcp_list[:, 2], gcp_list[:, 3], slave_geo)\n",
    "# Compute the the distance between the slave map coordinates and the master map coordinates for each GCP\n",
    "dist = np.sqrt((x_slave - gcp_list[:, 0]) ** 2 + (y_slave - gcp_list[:, 1]) ** 2)\n",
    "# Keep only those GCPs whose distance is lower than the threshold we set\n",
    "gcp_list = gcp_list[dist <= dist_threshold]\n",
    "# Convert back the list of GPC to a Python list\n",
    "gcp_list = gcp_list.tolist()\n",
    "good_matches = np.array(good_matches)[dist <= dist_threshold].tolist()\n",
    "print(f'Got {len(gcp_list)} valid GPCs with a translation lower than {dist_threshold}m')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the potential GCP filtered by distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.drawMatches(master_scaled, kp_master, slave_scaled, kp_slave, good_matches, img_matches,\n",
    "                matchesThickness=10, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "#-- Show detected matches\n",
    "plt.imshow(img_matches)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{seealso}\n",
    ":class:dropdown\n",
    "You can check [this function in the GitHub source code](https://github.com/hectornieto/airborne_tools/blob/5db17192e638c2745dea5d918b9dcaffd05a14cf/airborne_tools/gcp_tools.py#L568)\n",
    ":::\n",
    "\n",
    "## Remove GCPs that are too close each other\n",
    "On the other hand, we wanted to have well distributed GCPs across the whole image. Theferore we will discard GCPs that were very close to other GCPs, aiming to avoid overrepresnting one area over others:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set minimum distance in pixels that GCPs should be from other GCPs\n",
    "pixel_threshold = 5\n",
    "\n",
    "gcps_good = []\n",
    "matches_good = []\n",
    "for i, gcp_test in enumerate(gcp_list):\n",
    "    good = True\n",
    "    if i == len(gcp_list) - 2:\n",
    "        continue\n",
    "    for j in range(i + 1, len(gcp_list)):\n",
    "        dist = np.sqrt((gcp_test[2] - gcp_list[j][2]) ** 2 + (gcp_test[3] - gcp_list[j][3]) ** 2)\n",
    "        if dist < pixel_threshold:  # GCPs closer to each other are discarded to avoid overfitting\n",
    "            good = False\n",
    "            break\n",
    "    if good:\n",
    "        gcps_good.append(gcp_test)\n",
    "        matches_good.append(good_matches[i])\n",
    "\n",
    "gcp_list = list(gcps_good)\n",
    "good_matches = list(matches_good)\n",
    "print(f'Got {len(gcp_list)} valid GPCs far enough from each other by a distance of {dist_threshold} pixels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the filtered GCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.drawMatches(master_scaled, kp_master, slave_scaled, kp_slave, good_matches, img_matches,\n",
    "                matchesThickness=10, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "#-- Show detected matches\n",
    "plt.imshow(img_matches)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{seealso}\n",
    ":class:dropdown\n",
    "You can check [this function in the GitHub source code](https://github.com/hectornieto/airborne_tools/blob/5db17192e638c2745dea5d918b9dcaffd05a14cf/airborne_tools/gcp_tools.py#L594)\n",
    ":::\n",
    "\n",
    "## Remove GCPs that are outliers in a 3rd degree polynomial warp\n",
    "To finalize, we will discard all the GCPs that are outliers after fitting at 3rd degree polynomial warp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a warping error threshold of 2.5 cm\n",
    "warp_threshold = 0.025\n",
    "\n",
    "# Convert the list of GPCs to a python numpy array to vectorize the calculations\n",
    "gcps = np.asarray(gcp_list)\n",
    "good_matches = np.asarray(good_matches)\n",
    "\n",
    "# Create our 3rd degree polynomial warping helper function\n",
    "def _fit_polynomial_warp(gcps):\n",
    "\n",
    "    gcps = np.asarray(gcps)\n",
    "    if gcps.shape[0] < 15:\n",
    "        return None, None\n",
    "    rows = gcps[:, 2]\n",
    "    cols = gcps[:, 3]\n",
    "    rows2 = rows ** 2\n",
    "    cols2 = cols ** 2\n",
    "    rowscols = rows * cols\n",
    "    rows2cols = rows ** 2 * cols\n",
    "    rowscols2 = rows * cols ** 2\n",
    "    rows3 = rows ** 3\n",
    "    cols3 = cols ** 3\n",
    "\n",
    "    x = np.matrix([np.ones(rows.shape), rows, cols, rowscols, rows2, cols2, rows2cols, rowscols2, rows3, cols3]).T\n",
    "    map_x = gcps[:, 0].reshape(-1, 1)\n",
    "    map_y = gcps[:, 1].reshape(-1, 1)\n",
    "    theta_x = (x.T * x).I * x.T * map_x\n",
    "    theta_y = (x.T * x).I * x.T * map_y\n",
    "    return np.asarray(theta_x).reshape(-1), np.asarray(theta_y).reshape(-1)\n",
    "\n",
    "\n",
    "# Create our helper function to compute the warping error for each GCP\n",
    "def _calc_warp_erors(gcps, theta_x, theta_y):\n",
    "    def _polynomial_warp(rows, cols, theta_x, theta_y):\n",
    "        x = theta_x[0] + theta_x[1] * rows + theta_x[2] * cols + theta_x[3] * rows * cols + theta_x[4] * rows ** 2 + \\\n",
    "            theta_x[5] * cols ** 2 + theta_x[6] * rows ** 2 * cols + theta_x[7] * rows * cols ** 2 + \\\n",
    "            theta_x[8] * rows ** 3 + theta_x[9] * cols ** 3\n",
    "        y = theta_y[0] + theta_y[1] * rows + theta_y[2] * cols + theta_y[3] * rows * cols + theta_y[4] * rows ** 2 + \\\n",
    "            theta_y[5] * cols ** 2 + theta_y[6] * rows ** 2 * cols + theta_y[7] * rows * cols ** 2 + \\\n",
    "            theta_y[8] * rows ** 3 + theta_y[9] * cols ** 3\n",
    "        return x, y\n",
    "\n",
    "    gcps = np.asarray(gcps)\n",
    "    if len(gcps.shape) == 1:\n",
    "        gcps = gcps.reshape(1, -1)\n",
    "\n",
    "    rows = gcps[:, 2]\n",
    "    cols = gcps[:, 3]\n",
    "    x_model, y_model = _polynomial_warp(rows, cols, theta_x, theta_y)\n",
    "    error = np.sqrt((x_model - gcps[:, 0]) ** 2 + (y_model - gcps[:, 1]) ** 2)\n",
    "    return error\n",
    "\n",
    "# FIt a 3rd order polynomial warp\n",
    "theta_x, theta_y = _fit_polynomial_warp(gcps)\n",
    "error = _calc_warp_erors(gcps, theta_x, theta_y)\n",
    "while np.max(error) > warp_threshold and len(error) > 30:\n",
    "    index = error.argsort()[::-1]\n",
    "    gcps = gcps[index[1:]]\n",
    "    good_matches = good_matches[index[1:]]\n",
    "    theta_x, theta_y = _fit_polynomial_warp(gcps)\n",
    "    error = _calc_warp_erors(gcps, theta_x, theta_y)\n",
    "\n",
    "gcp_valid = list(gcps)\n",
    "good_matches = list(good_matches)\n",
    "print(f'Got {len(gcp_valid)} valid GPCs that fit well in a 3rd degree polynomial warp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the filtered GCPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.drawMatches(master_scaled, kp_master, slave_scaled, kp_slave, good_matches, img_matches,\n",
    "                matchesThickness=10, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "#-- Show detected matches\n",
    "plt.imshow(img_matches)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{seealso}\n",
    ":class:dropdown\n",
    "You can check [this function in the GitHub source code](https://github.com/hectornieto/airborne_tools/blob/5db17192e638c2745dea5d918b9dcaffd05a14cf/airborne_tools/gcp_tools.py#L761)\n",
    ":::\n",
    "\n",
    "\n",
    "## Create and save the new collocated `slave` image\n",
    "After running the cell below a new image will be saved in the `/input/UAV` folder, with suffix `_collocated.tif`. You can visualize the results on QGIS. In addition, in the `/input/UAV/GCS` you will have access to an ASCII table that summarizes the automatic GCPs found, and a line shapefile that represents the traslation vector from the original `slave` GCP coordinate to its corresponding coordinate in the `reference`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airborne_tools import gcp_tools as gcp\n",
    "# Set transformation method\n",
    "# 0 for thin plate spline transformation, \n",
    "# 1 for 1st degree polynomial\n",
    "# 2 for 2nd degree polynomial\n",
    "# ...\n",
    "transform = 0  \n",
    "\n",
    "# Set the extent for the output image equal as the master image\n",
    "xmin, ymax = img.get_map_coordinates(0, 0, master_geo)\n",
    "xmax, ymin = img.get_map_coordinates(master_scaled.shape[1], master_scaled.shape[0], master_geo)\n",
    "output_extent = (xmin, ymin, xmax, ymax)\n",
    "\n",
    "collocated_image = test_dir / 'tir_odm_20220916_collocated.tif'\n",
    "\n",
    "# Get the origial GCP map coordinates\n",
    "slave_xcoord, slave_yCoord = img.get_map_coordinates(np.asarray(gcp_valid)[:, 2],\n",
    "                                                     np.asarray(gcp_valid)[:, 3],\n",
    "                                                     slave_geo)\n",
    "\n",
    "# Create an anciillary GCP subfolder to store the final GPCs\n",
    "if not (test_dir / \"GCPs\").is_dir():\n",
    "    (test_dir / \"GCPs\").mkdir(parents=True)\n",
    "\n",
    "# Save the transformation line vector betwen the original coordinates and the collocated coordiantes\n",
    "outshapefile = test_dir / 'GCPs' / f\"{collocated_image.name[:-4]}_Transform.shp\"\n",
    "gcp._write_transformation_vector((slave_xcoord, slave_yCoord),\n",
    "                                 (np.asarray(gcp_valid)[:, 0], np.asarray(gcp_valid)[:, 1]),\n",
    "                                 outshapefile,\n",
    "                                 proj)\n",
    "\n",
    "# Write the GCP to ascii file\n",
    "outtxtfile = test_dir / 'GCPs' / f\"{collocated_image.name[:-4]}_GCPs.txt\"\n",
    "gcp.gcps_to_ascii(gcp_valid, outtxtfile)\n",
    "\n",
    "# Finally use a airbone_tools helper function to perform the reprojection with the GCPs\n",
    "gcp.warp_image_with_gcps(lst_file,\n",
    "                         gcp_valid,\n",
    "                         collocated_image,\n",
    "                         output_extent=None,\n",
    "                         src_no_data=lst_no_data,\n",
    "                         transform=transform,\n",
    "                         data_type=gdal.GDT_UInt16)\n",
    "\n",
    "print(f\"Saved collocated image in f{collocated_image}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{seealso}\n",
    ":class: dropdown\n",
    "You can check the full code in the [airborne_tools GitHub source code repository](https://github.com/hectornieto/airborne_tools/blob/5db17192e638c2745dea5d918b9dcaffd05a14cf/airborne_tools/gcp_tools.py#L18)\n",
    ":::\n",
    "\n",
    ":::{hint}\n",
    "\n",
    "Have a look in QGIS at the collocated TIR image comapred to the uncorrected TIR and Sequoia Multispectral image to visualize the colocation change. \n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "* We introduced an automatic method to corregister two mosaics\n",
    "* This method does not require that both mosaics share the same resolution nor the same extension, as SIFT tries to find scale-invariant features in the images.\n",
    "* After running SIFT+FLANN we run serveral filter criteria in order to use only the best GCPs.\n",
    "* [...]\n",
    "\n",
    ":::{note}\n",
    "Please feel free to comment any thoughts. This is work in progress!!!\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "toc": {
   "base_numbering": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
